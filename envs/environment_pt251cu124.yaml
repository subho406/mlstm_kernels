name: xlstmpt240cu124
channels:
  - pytorch
  - nvidia
  - conda-forge
  - defaults  # exclude this to work on leonardo
dependencies:
  - cuda=12.4
  - cuda-nvcc=12.4
  - gxx_linux-64=13.2.0
  - python=3.11
  - pip
  - pytorch=2.5.1
  - pytorch-cuda=12.4
  - torchvision
  - cmake
  - ninja
  - cuda-toolkit=12.4
  - cuda-cccl=12.4
  - pip:
      - einops #==0.7.0
      - opt_einsum #==3.3.0
      - transformers #==4.37.2  # replace this with xlstm transformers
      - datasets #==2.16.1
      - pre-commit #==3.6.0
      - reportlab #==4.0.9
      - lm-eval #==0.4.0
      - joypy #==0.2.6
      - ipykernel #==6.29.0
      - dacite #==1.8.1
      - ftfy #==6.1.3
      - ninja #==1.11.1.1
      - huggingface-hub #==0.20.3
      - joblib #==1.3.2
      - lightning #==2.1.3
      - rich #==13.7.0
      - lm-dataformat #==0.0.20
      - omegaconf #==2.3.0
      - sentencepiece #==0.1.99
      - tokenizers #==0.15.1
      - torchmetrics #==1.3.0
      - tqdm #==4.66.1
      - wandb #==0.16.2
      - seaborn #==0.13.2
      - pytest #==8.0.0
      - pytest-xdist #==3.5.0
      - openpyxl #==3.1.2
      - gitpython #==3.1.41
      - scipy #==1.12.0
      - pykeops #==2.2.1
      - hydra-core #==1.3.2
      - torchtext #==0.16.2
      - tensorboard #==2.15.1
      - cryptography #==42.0.2
      - tensorflow
      - numpy #==1.26.4
      - mamba-ssm
      - causal_conv1d
      - git+https://github.com/sustcsonglin/flash-linear-attention


      # Please install by hand
      # - packaging
      # - mamba-ssm #==1.0.1
      # - causal-conv1d #==1.1.1
      # - git+https://github.com/idiap/fast-transformers@master
      # - flash-attn
